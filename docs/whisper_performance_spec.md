# Whisper æ•ˆèƒ½å„ªåŒ–è¦æ ¼æ›¸

## æ¦‚è¿°

æœ¬æ–‡æª”å®šç¾© video-translate-project ä¸­ Whisper èªžéŸ³è­˜åˆ¥æ¨¡çµ„çš„æ•ˆèƒ½å„ªåŒ–æ–¹æ¡ˆï¼Œç›®æ¨™æ˜¯å°‡è½‰éŒ„é€Ÿåº¦æå‡ **4-8 å€**ã€‚

---

## ç¾æ³åˆ†æž

### ç•¶å‰é…ç½® (`translation_config.json`)

```json
{
  "whisper": {
    "model": "base",
    "language": "en",
    "device": "auto",
    "max_words_per_segment": 8
  }
}
```

### æ•ˆèƒ½ç“¶é ¸

| ç“¶é ¸é¡žåž‹ | èªªæ˜Ž | å½±éŸ¿ç¨‹åº¦ |
|---------|------|---------|
| æ¨¡åž‹è¼‰å…¥ | æ¯æ¬¡è™•ç†é‡æ–°è¼‰å…¥æ¨¡åž‹ | ä¸­ |
| æŽ¨è«–é€Ÿåº¦ | openai-whisper æœªå„ªåŒ– | **é«˜** |
| GPU åˆ©ç”¨çŽ‡ | æœªä½¿ç”¨ INT8/FP16 é‡åŒ– | é«˜ |
| æ‰¹æ¬¡è™•ç† | é€æª”è™•ç†ï¼Œç„¡ä¸¦è¡Œ | ä¸­ |
| I/O ç­‰å¾… | ç¿»è­¯ API ç­‰å¾…æ™‚é–“ | ä¸­ |

### åŸºæº–æ•ˆèƒ½ (ä¼°ç®—)

| å½±ç‰‡é•·åº¦ | ç•¶å‰è€—æ™‚ (CPU) | ç•¶å‰è€—æ™‚ (GPU) |
|---------|---------------|---------------|
| 1 åˆ†é˜ | ~60 ç§’ | ~15 ç§’ |
| 5 åˆ†é˜ | ~300 ç§’ | ~75 ç§’ |
| 10 åˆ†é˜ | ~600 ç§’ | ~150 ç§’ |

---

## å„ªåŒ–æ–¹æ¡ˆ

### æ–¹æ¡ˆä¸€ï¼šfaster-whisper é·ç§» (æŽ¨è–¦)

#### æŠ€è¡“åŽŸç†

`faster-whisper` ä½¿ç”¨ **CTranslate2** å¼•æ“Žï¼Œç›¸æ¯” openai-whisperï¼š

- **4x æ›´å¿«** çš„æŽ¨è«–é€Ÿåº¦
- **2x æ›´å°‘** çš„è¨˜æ†¶é«”ä½¿ç”¨
- æ”¯æ´ **INT8 é‡åŒ–** (CPU/GPU)
- æ”¯æ´ **æ‰¹æ¬¡è™•ç†**

#### æ•ˆèƒ½å°æ¯”

| æŒ‡æ¨™ | openai-whisper | faster-whisper | æå‡ |
|------|---------------|----------------|------|
| æŽ¨è«–é€Ÿåº¦ | 1x | 4-8x | ðŸš€ |
| VRAM ä½¿ç”¨ | 100% | ~50% | âœ… |
| CPU æ•ˆèƒ½ | æ…¢ | INT8 åŠ é€Ÿ | âœ… |
| é¦–æ¬¡è¼‰å…¥ | æ…¢ | å¿« | âœ… |

#### API å°æ¯”

**ç¾æœ‰ä»£ç¢¼ (openai-whisper):**
```python
import whisper
model = whisper.load_model("base", device="cuda")
result = model.transcribe(
    video_path,
    language="en",
    task="transcribe",
    verbose=False,
    word_timestamps=True
)
```

**æ–°ä»£ç¢¼ (faster-whisper):**
```python
from faster_whisper import WhisperModel

model = WhisperModel(
    "base",
    device="cuda",
    compute_type="float16"  # æˆ– "int8" for CPU
)

segments, info = model.transcribe(
    video_path,
    language="en",
    task="transcribe",
    word_timestamps=True,
    vad_filter=True,  # èªžéŸ³æ´»å‹•åµæ¸¬ï¼Œè·³éŽéœéŸ³
    vad_parameters=dict(min_silence_duration_ms=500)
)

# æ³¨æ„ï¼šsegments æ˜¯ generatorï¼Œéœ€è¦è¿­ä»£
for segment in segments:
    print(f"[{segment.start:.2f}s -> {segment.end:.2f}s] {segment.text}")
```

#### å®‰è£éœ€æ±‚

```bash
# ç§»é™¤èˆŠç‰ˆ
pip uninstall openai-whisper

# å®‰è£ faster-whisper
pip install faster-whisper

# GPU æ”¯æ´ (éœ€è¦ cuDNN å’Œ cuBLAS)
# Windows: è‡ªå‹•å¾ž PyPI ä¸‹è¼‰ CUDA åº«
# Linux: éœ€è¦å®‰è£ CUDA toolkit
```

#### compute_type é¸é …

| compute_type | è¨­å‚™ | é€Ÿåº¦ | ç²¾åº¦ | VRAM |
|-------------|------|------|------|------|
| `float32` | CPU/GPU | æœ€æ…¢ | æœ€é«˜ | é«˜ |
| `float16` | GPU | å¿« | é«˜ | ä¸­ |
| `int8_float16` | GPU | æ›´å¿« | ä¸­ | ä½Ž |
| `int8` | CPU/GPU | æœ€å¿« | ä¸­ | æœ€ä½Ž |

**æŽ¨è–¦é…ç½®ï¼š**
- GPU: `float16` æˆ– `int8_float16`
- CPU: `int8`

---

### æ–¹æ¡ˆäºŒï¼šGPU å„ªåŒ–

#### 2.1 ç¢ºä¿ CUDA å•Ÿç”¨

```python
# æª¢æ¸¬è…³æœ¬
import torch

def check_gpu():
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
```

#### 2.2 Windows CUDA å®‰è£

```bash
# å®‰è£ CUDA ç‰ˆæœ¬ PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# é©—è­‰å®‰è£
python -c "import torch; print(torch.cuda.is_available())"
```

#### 2.3 GPU è¨˜æ†¶é«”ç®¡ç†

```python
import torch

# æ¸…ç† GPU è¨˜æ†¶é«”
torch.cuda.empty_cache()

# è¨­å®šè¨˜æ†¶é«”åˆ†é…å™¨
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
```

#### 2.4 æ¨¡åž‹é¸æ“‡ vs VRAM

| æ¨¡åž‹ | åƒæ•¸é‡ | VRAM éœ€æ±‚ | é€Ÿåº¦ | æº–ç¢ºåº¦ |
|------|-------|----------|------|-------|
| tiny | 39M | ~1 GB | æœ€å¿« | ä½Ž |
| base | 74M | ~1.5 GB | å¿« | ä¸­ |
| small | 244M | ~2.5 GB | ä¸­ | é«˜ |
| medium | 769M | ~5 GB | æ…¢ | æ›´é«˜ |
| large-v3 | 1550M | ~10 GB | æœ€æ…¢ | æœ€é«˜ |

---

### æ–¹æ¡ˆä¸‰ï¼šä¸¦è¡Œè™•ç†æž¶æ§‹

#### 3.1 è™•ç†æµç¨‹åˆ†æž

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è™•ç†æµç¨‹ (ç•¶å‰)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Video 1: [===è½‰éŒ„===][==ç¿»è­¯==][=è‰ç¨¿=]                      â”‚
â”‚  Video 2:                       [===è½‰éŒ„===][==ç¿»è­¯==][=è‰ç¨¿=]â”‚
â”‚  Video 3:                                            [===... â”‚
â”‚                                                              â”‚
â”‚  æ™‚é–“è»¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è™•ç†æµç¨‹ (å„ªåŒ–å¾Œ)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  è½‰éŒ„ (GPU): [V1][V2][V3]  â† å¾ªåº (GPU è³‡æºç¨ä½”)              â”‚
â”‚  ç¿»è­¯ (API): [V1][V2][V3]  â† ä¸¦è¡Œ (I/O bound)                â”‚
â”‚  è‰ç¨¿ (I/O): [V1][V2][V3]  â† ä¸¦è¡Œ (I/O bound)                â”‚
â”‚                                                              â”‚
â”‚  Pipeline:                                                   â”‚
â”‚  V1: [è½‰éŒ„]                                                  â”‚
â”‚  V2:       [è½‰éŒ„]                                            â”‚
â”‚  V1:       [ç¿»è­¯]                                            â”‚
â”‚  V3:             [è½‰éŒ„]                                      â”‚
â”‚  V2:             [ç¿»è­¯]                                      â”‚
â”‚  V1:             [è‰ç¨¿]                                      â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.2 Pipeline æž¶æ§‹

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from queue import Queue
from dataclasses import dataclass
from typing import Optional
import threading

@dataclass
class VideoTask:
    video_path: str
    subtitles: Optional[list] = None
    translated: Optional[list] = None
    draft_path: Optional[str] = None
    status: str = "pending"

class TranscriptionPipeline:
    """
    ä¸‰éšŽæ®µ Pipeline æž¶æ§‹ï¼š
    1. è½‰éŒ„ (GPU-bound): å¾ªåºè™•ç†ï¼Œé¿å… GPU ç«¶çˆ­
    2. ç¿»è­¯ (I/O-bound): ä¸¦è¡Œè™•ç†ï¼Œæœ€å¤§åŒ– API åžåé‡
    3. è‰ç¨¿ç”Ÿæˆ (I/O-bound): ä¸¦è¡Œè™•ç†
    """

    def __init__(self, config: dict):
        self.config = config
        self.transcribe_queue = Queue()
        self.translate_queue = Queue()
        self.draft_queue = Queue()

        # ç¿»è­¯å’Œè‰ç¨¿ç”Ÿæˆå¯ä¸¦è¡Œ
        self.translate_workers = config.get("parallel", {}).get("translate_workers", 4)
        self.draft_workers = config.get("parallel", {}).get("draft_workers", 2)

    async def process_batch(self, video_paths: list) -> list:
        """Pipeline æ‰¹æ¬¡è™•ç†"""

        tasks = [VideoTask(path) for path in video_paths]

        # Stage 1: è½‰éŒ„ (å¾ªåº)
        for task in tasks:
            task.subtitles = await self._transcribe(task.video_path)
            task.status = "transcribed"
            self.translate_queue.put(task)

        # Stage 2: ç¿»è­¯ (ä¸¦è¡Œ)
        with ThreadPoolExecutor(max_workers=self.translate_workers) as executor:
            translate_futures = []
            while not self.translate_queue.empty():
                task = self.translate_queue.get()
                future = executor.submit(self._translate_sync, task)
                translate_futures.append(future)

            for future in translate_futures:
                task = future.result()
                task.status = "translated"
                self.draft_queue.put(task)

        # Stage 3: è‰ç¨¿ç”Ÿæˆ (ä¸¦è¡Œ)
        with ThreadPoolExecutor(max_workers=self.draft_workers) as executor:
            draft_futures = []
            while not self.draft_queue.empty():
                task = self.draft_queue.get()
                future = executor.submit(self._generate_draft_sync, task)
                draft_futures.append(future)

            results = [future.result() for future in draft_futures]

        return results
```

#### 3.3 ä¸¦è¡Œé…ç½®

```json
{
  "parallel": {
    "enabled": true,
    "mode": "pipeline",
    "transcribe_workers": 1,
    "translate_workers": 4,
    "draft_workers": 2,
    "max_concurrent_videos": 8
  }
}
```

#### 3.4 å·¥ä½œé¡žåž‹åˆ†æž

| å·¥ä½œé¡žåž‹ | è³‡æºç“¶é ¸ | ä¸¦è¡Œç­–ç•¥ | å»ºè­° Workers |
|---------|---------|---------|-------------|
| è½‰éŒ„ | GPU/CPU | å¾ªåº | 1 |
| ç¿»è­¯ | Network I/O | é«˜åº¦ä¸¦è¡Œ | 4-8 |
| è‰ç¨¿ç”Ÿæˆ | Disk I/O | ä¸­åº¦ä¸¦è¡Œ | 2-4 |

---

## å¯¦ä½œè¦æ ¼

### é…ç½®çµæ§‹æ›´æ–°

```json
{
  "whisper": {
    "engine": "faster-whisper",
    "model": "base",
    "language": "en",
    "device": "auto",
    "compute_type": "float16",
    "max_words_per_segment": 8,
    "vad_filter": true,
    "vad_parameters": {
      "min_silence_duration_ms": 500,
      "speech_pad_ms": 400
    }
  },
  "parallel": {
    "enabled": true,
    "mode": "pipeline",
    "translate_workers": 4,
    "draft_workers": 2
  },
  "performance": {
    "cache_model": true,
    "batch_size": 16,
    "prefetch_videos": 2
  }
}
```

### æ¨¡çµ„æ›´æ–°

#### `subtitle_generator.py` ä¿®æ”¹

```python
class SubtitleGenerator:
    """æ”¯æ´ openai-whisper å’Œ faster-whisper é›™å¼•æ“Ž"""

    def __init__(self, config: dict):
        self.config = config
        self.engine = config.get("whisper", {}).get("engine", "openai-whisper")
        self.model = None

    def _load_model(self):
        """å»¶é²è¼‰å…¥æ¨¡åž‹"""
        if self.model is not None:
            return self.model

        whisper_config = self.config.get("whisper", {})
        model_name = whisper_config.get("model", "base")
        device = self._get_device()

        if self.engine == "faster-whisper":
            from faster_whisper import WhisperModel
            compute_type = whisper_config.get("compute_type", "float16")
            self.model = WhisperModel(model_name, device=device, compute_type=compute_type)
        else:
            import whisper
            self.model = whisper.load_model(model_name, device=device)

        return self.model

    def _get_device(self) -> str:
        """è‡ªå‹•åµæ¸¬æœ€ä½³è¨­å‚™"""
        device = self.config.get("whisper", {}).get("device", "auto")
        if device == "auto":
            import torch
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device

    def transcribe(self, video_path: str) -> list:
        """è½‰éŒ„å½±ç‰‡"""
        model = self._load_model()
        whisper_config = self.config.get("whisper", {})

        if self.engine == "faster-whisper":
            return self._transcribe_faster(model, video_path, whisper_config)
        else:
            return self._transcribe_openai(model, video_path, whisper_config)

    def _transcribe_faster(self, model, video_path: str, config: dict) -> list:
        """ä½¿ç”¨ faster-whisper è½‰éŒ„"""
        segments, info = model.transcribe(
            video_path,
            language=config.get("language", "en"),
            task="transcribe",
            word_timestamps=True,
            vad_filter=config.get("vad_filter", True),
            vad_parameters=config.get("vad_parameters", {})
        )

        entries = []
        for segment in segments:
            # è™•ç† word-level timestamps
            words = segment.words or []
            entry = SubtitleEntry(
                index=len(entries) + 1,
                start_time=segment.start,
                end_time=segment.end,
                text_original=segment.text.strip(),
                text_translated=""
            )
            entries.append(entry)

        return self._split_long_segments(entries, config.get("max_words_per_segment", 8))
```

---

## æ•ˆèƒ½é æœŸ

### å„ªåŒ–å¾Œæ•ˆèƒ½ä¼°ç®—

| å½±ç‰‡é•·åº¦ | ç•¶å‰è€—æ™‚ | å„ªåŒ–å¾Œ (GPU) | å„ªåŒ–å¾Œ (CPU) |
|---------|---------|-------------|-------------|
| 1 åˆ†é˜ | ~60 ç§’ | ~5 ç§’ | ~15 ç§’ |
| 5 åˆ†é˜ | ~300 ç§’ | ~20 ç§’ | ~60 ç§’ |
| 10 åˆ†é˜ | ~600 ç§’ | ~40 ç§’ | ~120 ç§’ |

### æ‰¹æ¬¡è™•ç†æ•ˆèƒ½ (10 å€‹ 5 åˆ†é˜å½±ç‰‡)

| æ¨¡å¼ | è€—æ™‚ | èªªæ˜Ž |
|------|------|------|
| ç•¶å‰å¾ªåº | ~50 åˆ†é˜ | æ¯å€‹å½±ç‰‡ 5 åˆ†é˜ |
| Pipeline ä¸¦è¡Œ | ~15 åˆ†é˜ | è½‰éŒ„å¾ªåº + ç¿»è­¯/è‰ç¨¿ä¸¦è¡Œ |
| faster-whisper + Pipeline | ~5 åˆ†é˜ | å…¨é¢å„ªåŒ– |

---

## å¯¦ä½œéšŽæ®µ

### Phase 1: faster-whisper æ•´åˆ
- [ ] å®‰è£ faster-whisper ä¾è³´
- [ ] æ›´æ–° SubtitleGenerator æ”¯æ´é›™å¼•æ“Ž
- [ ] æ›´æ–°é…ç½®æª”æ ¼å¼
- [ ] æ•ˆèƒ½åŸºæº–æ¸¬è©¦

### Phase 2: GPU å„ªåŒ–
- [ ] CUDA ç’°å¢ƒæª¢æ¸¬è…³æœ¬
- [ ] compute_type è‡ªå‹•é¸æ“‡
- [ ] VRAM ç›£æŽ§å’Œç®¡ç†
- [ ] å›žé€€æ©Ÿåˆ¶ (GPU å¤±æ•—æ™‚ç”¨ CPU)

### Phase 3: Pipeline ä¸¦è¡Œ
- [ ] TranscriptionPipeline é¡žå¯¦ä½œ
- [ ] Queue-based å·¥ä½œåˆ†æ´¾
- [ ] é€²åº¦è¿½è¹¤å’Œå›žå ±
- [ ] éŒ¯èª¤è™•ç†å’Œé‡è©¦

### Phase 4: æ¸¬è©¦èˆ‡èª¿å„ª
- [ ] æ•ˆèƒ½åŸºæº–æ¸¬è©¦è…³æœ¬
- [ ] ä¸åŒç¡¬é«”é…ç½®æ¸¬è©¦
- [ ] æœ€ä½³åƒæ•¸èª¿å„ª
- [ ] æ–‡æª”æ›´æ–°

---

## ç›¸å®¹æ€§è€ƒé‡

### å‘å¾Œç›¸å®¹

```json
{
  "whisper": {
    "engine": "openai-whisper"  // ä¿æŒåŽŸæœ‰è¡Œç‚º
  }
}
```

### æ¼¸é€²å¼é·ç§»

1. å…ˆéƒ¨ç½² faster-whisperï¼Œè¨­ç‚ºå¯é¸
2. æ”¶é›†æ•ˆèƒ½æ•¸æ“š
3. ç¢ºèªç©©å®šå¾Œè¨­ç‚ºé è¨­
4. æ£„ç”¨ openai-whisper æ”¯æ´

---

## ä¾è³´æ›´æ–°

```txt
# requirements_translation.txt (æ›´æ–°)

# èªžéŸ³è­˜åˆ¥ (äºŒé¸ä¸€)
# openai-whisper>=20231117  # èˆŠç‰ˆï¼Œä¿ç•™ç›¸å®¹
faster-whisper>=1.0.0        # æ–°ç‰ˆï¼ŒæŽ¨è–¦

# GPU æ”¯æ´
torch>=2.0.0
# Windows: pip install torch --index-url https://download.pytorch.org/whl/cu121

# ç¿»è­¯ API
openai>=1.0.0
```

---

## é™„éŒ„

### A. æ•ˆèƒ½æ¸¬è©¦è…³æœ¬

```python
# benchmark_whisper.py
import time
from pathlib import Path

def benchmark_whisper(video_path: str, engine: str = "faster-whisper"):
    """æ•ˆèƒ½åŸºæº–æ¸¬è©¦"""
    from subtitle_generator import SubtitleGenerator

    config = {
        "whisper": {
            "engine": engine,
            "model": "base",
            "device": "auto",
            "compute_type": "float16"
        }
    }

    generator = SubtitleGenerator(config)

    # é ç†±
    print("Warming up...")
    _ = generator.transcribe(video_path)

    # æ­£å¼æ¸¬è©¦
    print("Benchmarking...")
    times = []
    for i in range(3):
        start = time.time()
        _ = generator.transcribe(video_path)
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"  Run {i+1}: {elapsed:.2f}s")

    avg_time = sum(times) / len(times)
    print(f"\nAverage: {avg_time:.2f}s")
    return avg_time

if __name__ == "__main__":
    import sys
    video = sys.argv[1] if len(sys.argv) > 1 else "test_video.mp4"
    benchmark_whisper(video)
```

### B. GPU æª¢æ¸¬è…³æœ¬

```python
# check_gpu.py
def check_gpu_support():
    """æª¢æ¸¬ GPU æ”¯æ´ç‹€æ…‹"""
    print("=" * 50)
    print("GPU Support Check")
    print("=" * 50)

    # PyTorch CUDA
    try:
        import torch
        print(f"\nPyTorch: {torch.__version__}")
        print(f"CUDA available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDA version: {torch.version.cuda}")
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            props = torch.cuda.get_device_properties(0)
            print(f"VRAM: {props.total_memory / 1e9:.1f} GB")
            print(f"Compute capability: {props.major}.{props.minor}")
    except ImportError:
        print("PyTorch not installed")

    # CTranslate2 (faster-whisper backend)
    try:
        import ctranslate2
        print(f"\nCTranslate2: {ctranslate2.__version__}")
        print(f"CUDA support: {ctranslate2.get_cuda_device_count() > 0}")
    except ImportError:
        print("\nCTranslate2 not installed")

    print("=" * 50)

if __name__ == "__main__":
    check_gpu_support()
```

---

*æ–‡ä»¶ç‰ˆæœ¬: v1.0*
*å»ºç«‹æ—¥æœŸ: 2024-12-30*
*ä½œè€…: Claude Code*
